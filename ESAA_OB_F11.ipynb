{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbberylll/ESAA_OB/blob/main/ESAA_OB_F11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 인공 신경망의 한계와 딥러닝 출현\n",
        "\n",
        "- 퍼셉트론: 다수의 입력을 받아 하나의 신호를 출력하는 초기 신경망 모델임.\n",
        "\n",
        "- 단층 퍼셉트론의 한계:\n",
        "\n",
        "  - AND, OR 게이트처럼 선형 분리가 가능한 문제는 해결할 수 있음.\n",
        "\n",
        "  - 하지만 XOR 게이트처럼 입력 데이터가 비선형적으로 분리되어야 하는 문제는 학습 및 분류가 불가능함.\n",
        "\n",
        "- 다층 퍼셉트론 (MLP): XOR 문제(비선형 문제)를 해결하기 위해 단층 퍼셉트론 사이에 은닉층을 추가하여 고안됨.\n",
        "\n",
        "- 딥러닝 (Deep Learning): 입력층과 출력층 사이에 여러 개의 은닉층을 가진 신경망 구조를 의미함."
      ],
      "metadata": {
        "id": "3cTvuUuOLZt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 딥러닝 구조\n",
        "\n",
        "### 딥러닝의 핵심 구성 요소\n",
        "- 층 (Layer): 데이터를 처리하는 단계로, 입력층, 은닉층, 출력층으로 구분됨.\n",
        "\n",
        "- 가중치 (Weight): 각 노드(뉴런) 간의 연결 강도를 나타내는 값임. 입력 값이 결과에 미치는 영향력을 조절함.\n",
        "\n",
        "- 바이어스 (Bias): 가중합(가중치와 신호의 곱의 합)에 추가로 더해지는 상수 값임. 모델이 데이터에 더 잘 맞도록 미세 조정하는 역할을 함.\n",
        "\n",
        "- 가중합 (Weighted Sum): 입력 신호와 해당 가중치의 곱을 모두 합한 값임. '전달 함수'라고도 불림.\n",
        "\n",
        "- 함수 (Function): 신경망 내에서 특정 계산을 수행하며, 대표적으로 활성화 함수와 손실 함수가 있음.\n",
        "\n",
        "***\n",
        "### 활성화 함수 (Activation Function)\n",
        ": 전달 함수(가중합)에서 받은 값을 그대로 출력하지 않고, 특정 기준에 따라 비선형 형태로 변환하여 출력 값을 결정하는 함수임.\n",
        "\n",
        "1. 시그모이드 (Sigmoid) 함수:\n",
        "  - 선형 함수의 결과를 0과 1 사이의 비선형 값으로 변환함.\n",
        "\n",
        "  - 기울기 소멸(Vanishing Gradient) 문제가 발생하기 쉬워, 최근 딥러닝 모델의 은닉층에서는 잘 사용되지 않음.\n",
        "\n",
        "2. 하이퍼볼릭 탄젠트 (Tanh) 함수:\n",
        "  - 결과를 -1과 1 사이의 비선형 값으로 변환함.\n",
        "  - 시그모이드의 결괏값 평균이 0이 아닌 점을 해결하여 중심 값을 0으로 맞춤.\n",
        "\n",
        "3. 렐루 (ReLU) 함수:\n",
        "  - 입력(x)이 음수면 0을, 양수면 x를 그대로 출력함.\n",
        "  - 학습 속도가 매우 빠르고, 시그모이드의 기울기 소멸 문제가 발생하지 않음.\n",
        "  - 일반적으로 은닉층에서 가장 많이 사용됨.\n",
        "\n",
        "4. 리키 렐루 (Leaky ReLU) 함수:\n",
        "  - 렐루의 변형으로, 입력 값이 음수일 때 0이 아닌 매우 작은 수(예: 0.01*x)를 반환함.\n",
        "  - 음수 입력 시 뉴런이 완전히 비활성화되는(죽는) 문제를 해결함.\n",
        "\n",
        "5. 소프트맥스 (Softmax) 함수:\n",
        "  - 입력 값들을 0과 1 사이의 값으로 정규화하며, 출력 값들의 총합이 항상 1이 되도록 만듦.\n",
        "  - 주로 다중 클래스 분류 문제의 출력층에서 확률 값으로 변환할 때 사용됨.\n",
        "\n",
        "\n",
        "***\n",
        "### 손실 함수 (Loss Function)\n",
        ": 학습을 통해 얻은 모델의 예측 값과 실제 데이터 값이 얼마나 차이 나는지(오차)를 평가하는 지표임. 딥러닝은 이 손실 함수 값을 최소화하는 방향으로 학습됨.\n",
        "\n",
        "- 평균 제곱 오차 (MSE): 실제 값과 예측 값의 차이를 제곱하여 평균을 낸 값임. 주로 회귀 문제에서 사용됨.\n",
        "\n",
        "- 크로스 엔트로피 오차 (Cross-Entropy): 실제 값의 확률 분포와 예측 값의 확률 분포 간의 차이를 계산함. 분류 문제(특히 원-핫 인코딩 사용 시)에서 널리 사용됨."
      ],
      "metadata": {
        "id": "Oxq6zu7bU6Wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 학습 과정\n",
        "- 순전파 (Forward Propagation):\n",
        "  - 훈련 데이터가 입력층 -> 은닉층 -> 출력층 방향으로 이동하며 처리되는 과정임.\n",
        "  - 이 과정을 통해 최종 예측 값이 계산됨.\n",
        "\n",
        "역전파 (Backward Propagation):\n",
        "  - 순전파 결과 계산된 예측 값과 실제 값의 차이(손실)를 출력층 -> 은닉층 -> 입력층 방향으로 역으로 전파하는 과정임.\n",
        "  - 각 뉴런의 가중치가 손실에 얼마나 영향을 미쳤는지(미분) 계산하여, 손실을 줄이는 방향으로 가중치를 업데이트함."
      ],
      "metadata": {
        "id": "zBbM8uzXeZ8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 딥러닝의 문제점과 해결 방안\n",
        "1. 과적합 (Overfitting) 문제: 모델이 훈련(학습) 데이터에만 과도하게 최적화되어, 새로운 데이터(테스트 데이터)에 대해서는 성능이 떨어지는 현상임.\n",
        "  - 해결: 드롭아웃(Dropout). 학습 과정 중 각 층의 노드 중 일부를 임의로 비활성화(학습에서 제외)시킴.\n",
        "\n",
        "2. 기울기 소멸 (Vanishing Gradient) 문제: 역전파 과정에서 오차(기울기)가 하위 층으로 갈수록 점차 작아져 0에 가까워지고, 결국 가중치가 업데이트되지 않아 학습이 중단되는 현상임.\n",
        "  - 해결: 시그모이드나 하이퍼볼릭 탄젠트 함수 대신 렐루(ReLU) 계열의 활성화 함수를 사용함.\n",
        "\n",
        "3. 성능 저하 문제: 학습이 비효율적이거나 최적점에 도달하지 못하는 문제임.\n",
        "  - 해결: **확률적 경사 하강법(SGD)**이나 미니 배치 경사 하강법 등 개선된 경사 하강법을 사용함.\n",
        "\n",
        "\n",
        "***\n",
        "### 경사 하강법 및 옵티마이저\n",
        "1. 배치 경사 하강법 (BGD): 전체 훈련 데이터셋에 대한 오류를 한 번에 계산하여 가중치를 업데이트함. 안정적이지만 데이터가 많으면 학습이 매우 오래 걸림.\n",
        "\n",
        "2. 확률적 경사 하강법 (SGD): 임의로 선택한 데이터 1개에 대해 기울기를 계산하고 가중치를 업데이트함. 속도가 빠르지만 불안정할 수 있음.\n",
        "\n",
        "3. 미니 배치 경사 하강법 (Mini-batch SGD): 전체 데이터를 여러 개의 작은 **'미니 배치'**로 나누고, 각 배치의 기울기 평균을 구해 가중치를 업데이트함. BGD와 SGD의 장점을 절충한 방식으로, 가장 널리 쓰임.\n",
        "\n",
        "4. 옵티마이저 (Optimizer): 딥러닝 모델이 더 빠르고 안정적으로 손실 함수의 최솟값을 찾도록 돕는 알고리즘임.\n",
        "    - 학습 속도(Learning Rate)를 조절하는 방식 (예: 아다그라드, 알엠에스프롭)\n",
        "    - 운동량(Momentum)을 조절하는 방식 (예: 모멘텀, 네스테로프 모멘텀)\n",
        "    - 두 방식을 혼합한 방식 (예: 아담(Adam))"
      ],
      "metadata": {
        "id": "TJCa3p7Skz6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 딥러닝 사용의 이점\n",
        ": 자동 특성 추출 (Feature Extraction): 데이터의 중요한 특징을 찾아내 벡터로 변환하는 과정을 엔지니어가 수동으로 할 필요 없이, 딥러닝 알고리즘이 학습 과정에서 스스로 통합하여 수행함.\n",
        "\n",
        "빅데이터의 효율적 활용: 데이터의 양이 많고 복잡할수록 딥러닝이 더 강력한 패턴 인식 성능을 보임."
      ],
      "metadata": {
        "id": "Aj06UiofzEsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 딥러닝 알고리즘\n",
        "\n",
        "1. 심층 신경망 (DNN): 입력층과 출력층 사이에 다수의 은닉층을 포함하는 가장 기본적인 인공 신경망임. 비선형적 관계 학습에 강점이 있음.\n",
        "\n",
        "2. 합성곱 신경망 (CNN): 이미지 처리에 뛰어난 성능을 보이는 알고리즘임.\n",
        "\n",
        "  - **합성곱층(Convolutional Layer)**과 **풀링층(Pooling Layer)**을 포함함.\n",
        "\n",
        "  - 이미지의 공간적/지역적 정보를 유지하면서 복수의 필터를 통해 특징을 추출하고, 풀링을 통해 특징을 강화함. (예: LeNet-5, AlexNet, VGG, GoogLeNet, ResNet 등)\n",
        "\n",
        "3. 순환 신경망 (RNN): 시계열 데이터(주식, 언어, 음성 등 시간 흐름에 따라 변화하는 데이터) 학습에 적합함.\n",
        "  - 이전 시간의 정보를 현재 학습에 반영하는 순환 구조를 가짐.\n",
        "  - 고질적인 **기울기 소멸 문제(장기 의존성 문제)**가 있으며, 이를 해결하기 위해 LSTM이나 GRU 등이 도입됨.\n",
        "\n",
        "4. 제한된 볼츠만 머신 (RBM): 가시층(입력층)과 은닉층 두 개의 층으로만 구성되며, 같은 층의 노드끼리는 연결되지 않고 가시층-은닉층 간에만 연결됨.\n",
        "  - 차원 감소, 특성 학습, 사전 학습(Pre-training) 등 다양한 용도로 활용됨.\n",
        "\n",
        "5. 심층 신뢰 신경망 (DBN): 제한된 볼츠만 머신(RBM)을 여러 층으로 쌓아 올린 형태의 구조임.\n",
        "  - 각 RBM 층을 순서대로 미리 학습(사전 훈련)시킨 후 전체 네트워크를 구성함."
      ],
      "metadata": {
        "id": "_SNLAJxUzXIk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwMEx8p2KCZk"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden=torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.relu=torch.nn.ReLu(inplace=True)\n",
        "        self.out=torch.nn.Linear(n_hidden, n_output)\n",
        "        self.softmax=torch.nn.Softmax(dim=n_output)\n",
        "    def forward(self, x):\n",
        "        x=self.hidden(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.out(x)\n",
        "        x=self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jRK1DuKPYGqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "\n",
        "#loss_fn=torch.nn.MSELoss(reduction='sum')\n",
        "#y_pred=model(x)\n",
        "#loss=loss_fn(y_pred, y)"
      ],
      "metadata": {
        "id": "bYcfOCTmZLLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss=nn.CrossEntropyLoss()\n",
        "#input=torch.randn(5, 6, requires_grad=True)\n",
        "#target=torch.empty(3, dtype=torch.long).random_(5)\n",
        "#output=loss(input, target)\n",
        "#output.backward()"
      ],
      "metadata": {
        "id": "bxzdvsexeJes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutModel, self).__init__()\n",
        "        self.layer1=torch.nn.Linear(784, 1200)\n",
        "        self.dropout1=torch.nn.Dropout(0.5)\n",
        "        self.layer2=torch.nn.Linear(1200, 1200)\n",
        "        self.dropout2=torch.nn.Dropout(0.5)\n",
        "        self.layer3=torch.nn.Linear(1200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=F.relu(self.layer1(x))\n",
        "        x=self.dropout1(x)\n",
        "        x=F.relu(self.layer2(x))\n",
        "        x=self.dropout2(x)\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "03MOV8HWlEot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class CustomDataset(Dataset):\n",
        "#    def __init__(self):\n",
        "#        self.x_data=[[1,2,3], [4,5,6], [7,8,9]]\n",
        "#        self.y_data=[[12], [18], [11]]\n",
        "#        def __len__(self):\n",
        "#            return len(self.x_data)\n",
        "#        def __getitem__(self, idx):\n",
        "#            x=torch.FloatTensor(self.x_data[idx])\n",
        "#            y=torch.FloatTensor(self.y_data[idx])\n",
        "#            return x, y\n",
        "#dataset=CustomDataset()\n",
        "#dataloader=DataLoader(\n",
        "#    dataset,\n",
        "#    batch_size=2,\n",
        "#    shuffle=True,\n",
        "#)"
      ],
      "metadata": {
        "id": "dfyo9l0yyNtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}