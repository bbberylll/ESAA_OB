{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQkjno5A5u0g9qcx6II0Lj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbberylll/ESAA_OB/blob/main/ESAA_OB_F04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 분석\n",
        "\n",
        "### NLP vs Text Mining\n",
        ": NLP는 텍스트 분석을 정교하게 발전하게 해주는 기반 기술\n",
        "\n",
        ": 텍스트 분석은 비정형 데이터를 분석하는 것. 따라서 **비정형 텍스트 데이터를 피처 형태로 추출하고, 피처에 의미 있는 값을 부여하는 과정**이 필수적.\n",
        "  \n",
        "  * 피처 벡터화(=피처 추출)\n",
        "\n",
        "        텍스트를 word 기반을 피처로 추출 >> 단어 빈도수 부여\n",
        "        = 텍스트는 단어의 조합인 벡터값으로 표현됨\n",
        "\n",
        "      * 예시 : BOW, Word2Vec   \n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### 텍스트 분석의 종류\n",
        "1. 텍스트 분류\n",
        "2. 감성 분석\n",
        "3. 텍스트 요약\n",
        "4. 텍스트 군집화와 유사도 측정\n",
        "\n",
        "### 텍스트 분석 과정\n",
        "1. 텍스트 사전 준비작업(=텍스트 전처리)\n",
        "  * 클렌징, 대/소문자 변경, 특수문자 삭제 / 토큰화 작업 / 의미 없는 단어 제거 작업 / 어근 추출(Stemming, Lemmatization)\n",
        "\n",
        "2. 피처 벡터화\n",
        "  * BOW(Count 기반, TF-IDF 기반 벡터화), Word2Vec\n",
        "3. ML 모델 수립 및 학습/예측/평가\n",
        "\n",
        "\n",
        "### 파이썬 기반 NLP, 텍스트 분석 패키지\n",
        ": NLTK, gensim, spaCy\n",
        "  - NLTK는 굉장히 방대함. 다만 수행 속도가 아쉬운 측면이 있음. 실용적이지 않은 편\n",
        "  - Gensim : 토픽 모델링 분야에서 자주 활용됨. Word2Vec 구현 가능\n",
        "  - SpaCy : 수행 성능이 뛰어남\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### 텍스트 분석 사전 준비 작업 = 텍스트 전처리 = 텍스트 정규화\n",
        "1. 클렌징 : 불필요한 문자, 기호 제거\n",
        "2. 토큰화 : 문장 토큰화 & 단어 토큰화 -- API 활용\n",
        "\n",
        "    *BOW는 단어의 순서가 중요하지 않음.\n",
        "    그래서 단어 토큰화만 사용하기도 함\n",
        "    문장 토큰화는 각 문장의 semantic 의미가 중요할 때 사용*\n",
        "    * 문장 토큰화 : 문장의 끝을 의미하는 기호를 찾아냄\n",
        "      * ex) NLTK : sent_tokenize 활용\n",
        "    * 단어 토큰화 : 공백, 콤마, 마침표, 정규 표현식 등을 이용해 단어를 분리\n",
        "      * ex) word_tokenize()\n",
        "\n",
        "    > 이렇게 문장을 단어 단위로 쪼갤 경우 문맥적인 의미가 무시되는 문제 발생\n",
        "    ---> n-gram이 도입됨. 연속된 n개의 단어를 하나로 토큰화 함\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. 필터링 / Stop word 제거 / 철자 수정\n",
        "\n",
        "    Stop word 제거 : 분석에 의미 없는 단어를 제거하는 과정. 언어별로 Stop word가 목록화 돼 있음 활용하면 된다.\n",
        "\n",
        "\n",
        "4. Stemming & Lemmatization : 단어의 원형을 찾아가는 과정\n",
        "  - Lemmatization이 더 정교한 결과를 제공함\n",
        "\n",
        "    * NLTK : Porter, Lancaster, Snowball Stemmer // WordNetLemmatizer 제공\n",
        "\n",
        "\n",
        "\n",
        "## BOW = Bag of Words\n",
        ": 단어가 가지는 순서, 맥락을 무시하고 frequency 값을 부여해 피처 추출하는 모델\n",
        "\n",
        "* 장점\n",
        "  * 쉽고 빠른 구축\n",
        "* 단점\n",
        "  * 문맥 의미 (Semantic Context) 반영 부족. n_gram으로도 부족함\n",
        "  * 희소 행렬 문제 : 한 문서에 수만~수십만개의 단어가 나타나는데, 문서마다 다른 단어가 나타남 --> 희소 행렬이 나타날 가능성이 매우 높음\n",
        "\n",
        "### BOW 피처 벡터화\n",
        "\n",
        "1. 정의 :\n",
        "텍스트를 특정 의미를 가지는 벡터 값으로 변환하는 과정 = 피처 벡터화\n",
        "BOW에서는 모든 문서에서 모든 단어를 칼럼으로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여함\n",
        "\n",
        "2. 종류 : count 기반의 벡터화 VS TF-IDF (Term Freq Inverse Document Freq)\n",
        "  - TF-IDF는 모든 문서에서 전반적으로 많이 나타나는 단어에 페널티를 줌!!\n",
        "  - 단순히 많이 나타난다고 점수를 잘 주는게 아니라, 이 문서에서 유난히 많이 나타나는 단어를 확인하고자 하는 것!!\n",
        "\n",
        "3. 구현 방법 : CountVectorize, TfidfVectorize 클래스\n",
        "  - CountVectorize 클래스 : 소문자 일괄 변환, 토큰화, 스톱 워드 필터링 + 피처벡터화 진행 & fit, transform 이용\n",
        "    * 관련 파라미터\n",
        "      * max_df : 너무 높은 빈도수인 단어 피처 제외하기 위한 freq high limit\n",
        "        * 전체에서 공통적으로 나타나는 단어면 무의미할 확률 증가\n",
        "      * min_df : 너무 낮은 빈도수인 단어 피처 제외하려는 freq low limit\n",
        "      * max_features : 추출하는 피처의 개수를 제한\n",
        "      * stop_words : 스톱 워드 파일 언어 지정\n",
        "\n",
        "\n",
        "4. CountVectorize, TfidfVectorize 과정\n",
        "  1) 영어면 소문자로 변경하는 과정 수행\n",
        "  2) 디폴트 단어 기준으로 n_gram_range를 반영해 단어 토큰화 진행\n",
        "  3) 텍스트 정규화 수행 : stop words, 어근 추출\n",
        "  4) 피처 벡터화 진행\n",
        "\n",
        "\n",
        "### BOW 희소행렬 변환\n",
        "1. 희소행렬 - COO 형식\n",
        ": 좌표 형식. 0이 아닌 데이터만 별도의 데이터 array에 저장하고 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장함\n",
        "\n",
        "2. 희소행렬 - CSR 형식\n",
        ": Compressed Sparse Row. 행이 바뀌는 지점만을 뽑아내서 저장하고 마지막에 데이터의 총 항목 개수를 추가."
      ],
      "metadata": {
        "id": "4jOWh4ePF5cl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRskp7OnFwlT",
        "outputId": "7df5957c-8640-403c-d1f1-0296f04fbc5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "\n",
        "print(type(sentences),len(sentences))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwKsqcmTJUWz",
        "outputId": "f0f2413c-d808-4325-ec00-f8eb383e516d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsXQIXl9Keb1",
        "outputId": "848eeeea-6b05-452a-8aa0-c141f9d1fd89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HduWwfemLDAY",
        "outputId": "8f312ac9-d292-4261-c3af-128e2dba9058"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('영어 stop words 갯수 : ', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASFPdWGgLisC",
        "outputId": "ed065da1-67f7-4390-dd56-083c4784ad22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 갯수 :  198\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "  filtered_words =[]\n",
        "\n",
        "  for word in sentence:\n",
        "    word = word.lower() ## 소문자 변환\n",
        "\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "\n",
        "  all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHiX19GiLxsY",
        "outputId": "0dc96b7d-5d00-4292-de5f-fa5b21ee55c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPM-RMdPNAWo",
        "outputId": "7583495e-7c8b-4ee5-c26d-c61ef0909c55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('working','v'), lemma.lemmatize('works','v'), lemma.lemmatize('worked','v'))\n",
        "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efNnRNzfNUtO",
        "outputId": "37f315e9-38b1-467c-e845-9590ba74a9c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "dense = np.array([[3,0,1], [0,2,0]])"
      ],
      "metadata": {
        "id": "hLK_wlI7N0OJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "data = np.array([3,1,2])\n",
        "\n",
        "row_pos = np.array([0,0,1])\n",
        "col_pos = np.array([0,2,1])\n",
        "\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
      ],
      "metadata": {
        "id": "D9__DzB0SJXl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_coo.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO2M9Xu3SYEZ",
        "outputId": "98d9f437-d3e2-4e2c-8617-46fbfc6be222"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[[0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0], [2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j-PHoHWSaHx",
        "outputId": "67077e69-8fcb-4898-8a54-48da7fb9efdf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 5],\n",
              " [1, 4, 0, 3, 2, 5],\n",
              " [0, 6, 0, 3, 0, 0],\n",
              " [2, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 7, 0, 8],\n",
              " [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "                  [1,4,0,3,2,5],\n",
        "                  [0,6,0,3,0,0],\n",
        "                  [2,0,0,0,0,0],\n",
        "                  [0,0,0,7,0,8],\n",
        "                  [1,0,0,0,0,0]])\n",
        "\n",
        "\n",
        "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
        "\n",
        "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
        "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
        "\n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
        "\n",
        "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
        "\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print(\"COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\")\n",
        "print(sparse_coo.toarray())\n",
        "print(\"CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\")\n",
        "print(sparse_csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxSmgC14TW7q",
        "outputId": "aa2c5e0d-e9f5-4166-b0e4-bec0f77e326d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}